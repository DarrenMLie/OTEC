# -*- coding: utf-8 -*-
"""oceantemp_training2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mr4jbacnteVec0I6lYSRfIZEvg4wEogI

# Overview

**NOTE: Significant code is used from Prof. Bilionis's course - ME597: Data Analytics for Scientists and Engineers**
"""

## Import Libraries
import numpy
import random
import pandas as pd
import matplotlib.pyplot as plt
import os
import sklearn
import scipy
import torch
import torch.nn as nn
from numpy import linalg as LA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import gc
import zipfile
import scipy.stats as st
from pickle import dump
from pickle import load
from matplotlib import pyplot as plt
import wandb

wandb.login()

# Set Seeds
torch.manual_seed(175643)
random.seed(175643)
numpy.random.seed(175643)


def data_loader(datafile):
    """Function to load CSV data, remove unnecessary columns, split into training and validation datasets,
    randomize and save to disk
    """
    chunk = 100000
    test_size = 0.33

    tp = pd.read_csv(datafile, chunksize=chunk,
                     usecols=['Probe', 'Latitude', 'Longitude', 'Month', 'Depth', 'Temperature'],
                     dtype={'Probe': 'str', 'Latitude': 'float32', 'Longitude': 'float32'})
    prac_data = pd.concat(tp, ignore_index=True)
    prac_data.head()
    prac_data = prac_data[prac_data.Probe != "XBT"]
    prac_data = prac_data[prac_data.Temperature > -3]
    remove_cols = ['Probe']
    df = prac_data.drop(remove_cols, axis=1)
    df.dropna(inplace=True)
    del tp
    del prac_data
    gc.collect()
    df.sample(frac=1)
    data = df.to_numpy()
    del df
    gc.collect()
    X = data[:, :-1]
    y = data[:, -1][:, None]
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size)
    # Build the scalers
    feature_scaler = StandardScaler().fit(X_train)
    target_scaler = StandardScaler().fit(y_train)
    dump(feature_scaler, open('featureScaler.pkl', 'wb'))
    dump(target_scaler, open('targetScaler.pkl', 'wb'))
    # Get scaled versions of the data
    X_train_scaled = feature_scaler.transform(X_train)
    y_train_scaled = target_scaler.transform(y_train)
    X_val_scaled = feature_scaler.transform(X_val)
    y_val_scaled = target_scaler.transform(y_val)
    # del X_train
    # del y_train
    # del X_val
    # del y_val
    del X
    del y
    gc.collect()
    numpy.savetxt('PracticeData1YearsRandTrain.csv', numpy.concatenate((X_train_scaled, y_train_scaled), axis=1),
                  delimiter=',')
    del X_train_scaled
    del y_train_scaled
    gc.collect()
    numpy.savetxt('PracticeData1YearsRandXVal.csv', X_val_scaled, delimiter=',')
    numpy.savetxt('PracticeData1YearsRandYVal.csv', y_val_scaled, delimiter=',')
    del y_val_scaled
    del X_val_scaled
    gc.collect()
    return X_val, y_val


"""

# Training the network"""


class TrainedModel(object):
    """
    A class that represents a trained network model.
   
    Parameters:
    
    net            -    A network.
    standarized    -    True if the network expects standarized features and outputs
                        standarized targets. False otherwise.
    feature_scaler -    A feature scalar - Ala scikit learn. Must have transform()
                        and inverse_transform() implemented.
    target_scaler  -    Similar to feature_scaler but for targets...
    """

    def __init__(self, net, standarized=False, feature_scaler=None, target_scaler=None):
        self.net = net
        self.standarized = standarized
        self.feature_scaler = feature_scaler
        self.target_scaler = target_scaler

    def __call__(self, X):
        """
        Evaluates the model at X.
        """
        # If not scaled, then the model is just net(X)
        if not self.standarized:
            return self.net(X)
        # Otherwise:
        # Scale X:
        X_scaled = self.feature_scaler.transform(X)
        # Evaluate the network output - which is also scaled:
        y_scaled = self.net(torch.Tensor(X_scaled))
        # Scale the output back:
        y = self.target_scaler.inverse_transform(y_scaled.detach().numpy())
        return y


def build_network(layers, neurons, activation):
    """
  A function that builds a network with nn.append and nn.sequential.

  layers      -     number of fully connected layers
  neurons     -     number of neurons in each fully connected layer
  activation  -     nonlinear activation function 
  """

    # initialize activation function
    if activation == 'ReLU':
        nl_act = nn.ReLU()

    elif activation == 'Tanh':
        nl_act = nn.Tanh()

    elif activation == 'Sigmoid':
        nl_act = nn.Sigmoid()
    else:
        raise ValueError('Activation is not - ReLU, Tanh, or Sigmoid')

    # add layers to the network
    lay = []
    sizes = [10, 30, 50, 60, 70, 90, 90, 70, 60, 50, 30, 10]


    # input takes 4 variables (lat, long, depth, time)
    lay.append(nn.Linear(4, neurons))
    lay.append(nl_act)

    for l in range(len(sizes)-1):
        lay.append(nn.Linear(sizes[l], sizes[l+1]))
        lay.append(nl_act)

    # append fully connected layers with activation
    # for l in range(layers):
    #     lay.append(nn.Linear(neurons, neurons))
    #     lay.append(nl_act)

    # output gives 1 variable (temp)
    lay.append(nn.Linear(neurons, 1))

    net = nn.Sequential(*lay)
    return net


def build_optimizer(network, optimizer, lr):
    if optimizer == 'sgd':
        optim = torch.optim.SGD(network.parameters(), lr=lr, momentum=0.95)

    elif optimizer == 'adam':
        # set default optimizer to adam
        optim = torch.optim.Adam(network.parameters(), lr=lr)

    else:
        raise ValueError('Optimizer is not initialized to "sgd" or "adam"')

    return optim


def train_net(loader):
    """
    A function that trains a regression neural network using stochatic gradient
    descent and returns the trained network. The loss function being minimized is
    `loss_func`.
    
    Parameters:
    
    loader     -    Data for one large batch of data
    """

    for X_batch, y_batch in loader:
        # put all data on device
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        y_pred = net(X_batch).to(device)

        # zero the optimizer for each batch
        optimizer.zero_grad()

        # loss = loss_function(y_batch, y_pred, config.reg_weight, net.parameters())
        # compute the loss and step the optimizer
        loss = loss_function(y_batch, y_pred)
        loss.backward()
        optimizer.step()

        # log the batch loss
        wandb.log({"batch loss": loss.item()})
        batch_loss.append(loss.item())

    # Return everything we need to analyze the results
    return batch_loss, val_loss


epochs = 40
lr = 0.0005791192343766891
reg_weight = 1e-3
n_batch = 256
batchLarge = n_batch * 100

# Define the layers of the neural network. 
config = {
    'dataset': 'practice',  # create a nickname for full dataset here
    'epochs': epochs,
    'batch_size': n_batch,
    'learning_rate': lr,
    'reg_weight': reg_weight,
    'optimizer': 'adam',
    'layers': 14,
    'neurons': 10,
    'activation': 'ReLU'
}

# Process and load data
X_val_Nscale, y_val_Nscale = data_loader('practiceMonth.csv')

# initialize loss function
loss_function = nn.MSELoss()

# Experiment tracking with weights and biases
experiment_name = wandb.util.generate_id()
with wandb.init(project='OTEC', config=config):
    config = wandb.config  # build the network
    net = build_network(config.layers, config.neurons, config.activation)

    # put network on the GPU
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        net.cuda()
    else:
        device = torch.device('cpu')

    # build optimizer
    optimizer = build_optimizer(net, config.optimizer, config.learning_rate)

    val_loss = []
    batch_loss = []

    X_val = pd.read_csv('PracticeData1YearsRandXVal.csv')
    Y_val = pd.read_csv('PracticeData1YearsRandYVal.csv')
    X_validation = X_val.to_numpy()
    Y_validation = Y_val.to_numpy()
    y_val_scaled = torch.Tensor(Y_validation)
    new_shape = (len(Y_validation), 1)
    y_val_scaled = y_val_scaled.view(new_shape)
    X_val_scaled = torch.Tensor(X_validation)
    new_shape_x = (len(X_validation), 4)
    X_val_scaled = X_val_scaled.view(new_shape_x)
    del X_val
    del Y_val
    del X_validation
    del Y_validation
    gc.collect()

    for e in tqdm(range(config.epochs)):
        for batch in pd.read_csv('PracticeData1YearsRandTrain.csv', chunksize=batchLarge):
            data = batch.to_numpy()
            X = data[:, :4]
            Y = data[:, -1]
            X_train_scaled = torch.Tensor(X)
            new_shape_x_train = (len(X), 4)
            X_train_scaled = X_train_scaled.view(new_shape_x_train)
            y_train_scaled = torch.Tensor(Y)
            new_shape_y_train = (len(Y), 1)
            y_train_scaled = y_train_scaled.view(new_shape_y_train)
            train_dataset = torch.utils.data.TensorDataset(X_train_scaled, y_train_scaled)
            train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
            del X
            del Y
            del data
            del batch
            gc.collect()
            batch_loss, val_loss = train_net(train_data_loader)
        # Evaluate the test loss and append it on the list `test_loss`
        y_pred_val = net(X_val_scaled.to(device))
        v_loss = loss_function(y_val_scaled.to(device), y_pred_val.to(device))
        val_loss.append(v_loss.item())
        wandb.log({"validation loss": v_loss.item()})

    feature_scaler = load(open('featureScaler.pkl', 'rb'))
    target_scaler = load(open('targetScaler.pkl', 'rb'))

    trained_model = TrainedModel(net, standarized=True, feature_scaler=feature_scaler, target_scaler=target_scaler)

wandb.finish()

#Validation
epoch_list = range(epochs)
fig, ax = plt.subplots(dpi = 150)
ax.plot(epoch_list, val_loss)
ax.set_xlabel('Epochs')
ax.set_ylabel('Val Loss')
ax.set_title('Validation Performance')
plt.savefig('Val_loss.png')

y_pred = trained_model(X_val_Nscale)
fig, ax = plt.subplots(dpi = 100)
ax.plot(y_pred, y_val_Nscale,'bo')
ax.plot(y_val_Nscale,y_val_Nscale,'r-')
ax.set_xlabel('Predicted Temperature')
ax.set_ylabel('Measured Temperature')
ax.set_title('Model verification on training data set')
plt.savefig('ObservedvPredicted.png')


# save the weights of the scaled model. This requires both the network architecture and the TrainedModel class to use
path_saved_model = 'savemodel.pth'
torch.save(trained_model.net.state_dict(), path_saved_model)
