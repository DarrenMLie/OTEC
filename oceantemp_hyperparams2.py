# -*- coding: utf-8 -*-
"""oceantemp_hyperparams.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IAlUppSC4YcAbeX2SI5XhVyvwbqWt1E8

# Overview

**NOTE: Significant code is used from Prof. Bilionis's course - ME597: Data Analytics for Scientists and Engineers**
"""

## Import Libraries
import numpy
import random
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import wandb
import sys
import os
wandb.login()

#Set Seeds
torch.manual_seed(175643)
random.seed(175643)
numpy.random.seed(175643)

print(os.getcwd())
data = 'PracticeMonth.csv'
chunk = 100000
prac_data = pd.read_csv(data, usecols=['Probe', 'Latitude', 'Longitude', 'Month', 'Depth', 'Temperature'],
                 dtype= {'Probe': 'str', 'Latitude': 'float32', 'Longitude': 'float32', 'Month': 'int32',
                         'Depth': 'float32', 'Temperature': 'float32'})
#prac_data = pd.concat(tp, ignore_index=True)
#print(len(prac_data))
print(prac_data.shape)
prac_data = prac_data[prac_data.Probe != "XBT"]
prac_data = prac_data[prac_data.Temperature > -3]
prac_data.dropna(inplace=True)
# print(prac_data.isnull().sum())
# print(prac_data.shape)


## Split and shuffle the data into training and validation sets


data = prac_data.to_numpy()
X = data[:, :-1]
X = X[:,1:].astype(float)
y = data[:, -1][:, None]
# print(X.dtype)
# print(X[:4,:])
# print(X[numpy.isnan(X)])
# print(numpy.isnan(X).sum())


"""

# Training the network"""

class TrainedModel(object):
    """
    A class that represents a trained network model.
   
    Parameters:
    
    net            -    A network.
    standarized    -    True if the network expects standarized features and outputs
                        standarized targets. False otherwise.
    feature_scaler -    A feature scalar - Ala scikit learn. Must have transform()
                        and inverse_transform() implemented.
    target_scaler  -    Similar to feature_scaler but for targets...
    """

    def __init__(self, net, standarized=False, feature_scaler=None, target_scaler=None):
        self.net = net
        self.standarized = standarized
        self.feature_scaler = feature_scaler
        self.target_scaler = target_scaler

    def __call__(self, X):
        """
        Evaluates the model at X.
        """
        # If not scaled, then the model is just net(X)
        if not self.standarized:
            return self.net(X)
        # Otherwise:
        # Scale X:
        X_scaled = self.feature_scaler.transform(X)
        # Evaluate the network output - which is also scaled:
        y_scaled = self.net(torch.Tensor(X_scaled))
        # Scale the output back:
        y = self.target_scaler.inverse_transform(y_scaled.detach().numpy())
        return y


def build_network(layers, neurons, activation):
  """
  A function that builds a network with nn.append and nn.sequential.

  layers      -     number of fully connected layers
  neurons     -     number of neurons in each fully connected layer
  activation  -     nonlinear activation function 
  """

  # initialize activation function
  if activation == 'ReLU':
    nl_act = nn.ReLU()

  elif activation == 'Tanh':
    nl_act = nn.Tanh()

  elif activation == 'Sigmoid':
    nl_act = nn.Sigmoid()
  else:
    raise ValueError('Activation is not - ReLU, Tanh, or Sigmoid')

  # add layers to the network
  lay = []

  #input takes 4 variables (lat, long, depth, time)
  lay.append(nn.Linear(4, neurons))
  lay.append(nn.ReLU())

  # append fully connected layers with activation
  for l in range(layers):
    lay.append(nn.Linear(neurons, neurons))
    lay.append(nl_act)

  # output gives 1 variable (temp)
  lay.append(nn.Linear(neurons, 1))

  net = nn.Sequential(*lay)
  return net


def build_optimizer(network, optimizer, lr):
  if optimizer == 'sgd':
    optim = torch.optim.SGD(network.parameters(), lr= lr, momentum = 0.95)

  elif optimizer == 'adam':
    # set default optimizer to adam
    optim = torch.optim.Adam(network.parameters(), lr= lr)

  else:
    raise ValueError('Optimizer is not initialized to "sgd" or "adam"')

  return optim

def train_net(config = None, test_size=0.33, standarize=True):
    """
    A function that trains a regression neural network using stochatic gradient
    descent and returns the trained network. The loss function being minimized is
    `loss_func`.
    
    Parameters:
    
    X          -    The observed features
    y          -    The observed targets
    config     -    Contains all hyperparameters needed to train the model
      net        -    The network you want to fit
      n_batch    -    The batch size you want to use for stochastic optimization
      epochs     -    How many times do you want to pass over the training dataset.
      lr         -    The learning rate for the stochastic optimization algorithm.

    test_size  -    What percentage of the data should be used for testing (validation).
    standarize -    Whether or not you want to standarize the features and the targets.
    """
    # Split the data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size)

    # Standarize the data
    if standarize:
        # Build the scalers
        feature_scaler = StandardScaler().fit(X)
        target_scaler = StandardScaler().fit(y)
        # Get scaled versions of the data
        X_train_scaled = feature_scaler.transform(X_train)
        y_train_scaled = target_scaler.transform(y_train)
        X_val_scaled = feature_scaler.transform(X_val)
        y_val_scaled = target_scaler.transform(y_val)
    else:
        feature_scaler = None
        target_scaler = None
        X_train_scaled = X_train
        y_train_scaled = y_train
        X_val_scaled = X_val
        y_val_scaled = y_val

    # Turn all the numpy arrays to torch tensors
    X_train_scaled = torch.Tensor(X_train_scaled)
    X_val_scaled = torch.Tensor(X_val_scaled)
    y_train_scaled = torch.Tensor(y_train_scaled)
    y_val_scaled = torch.Tensor(y_val_scaled)

    # initialize loss function 
    loss_function = nn.MSELoss()

    # Experiment tracking with weights and biases 
    experiment_name = wandb.util.generate_id()

    # enable wandb to handle parameters
    with wandb.init(project = 'OTEC') as run:
      config = wandb.config

      #build the network
      net = build_network(config.layers, config.neurons, config.activation)

      # put network on the GPU
      if torch.cuda.is_available():
        device = torch.device("cuda:0")
        net.cuda()
      else:
        device = torch.device('cpu')

      # build optimizer
      optimizer = build_optimizer(net, config.optimizer, config.learning_rate)

      # create dataloader
      train_dataset = torch.utils.data.TensorDataset(X_train_scaled, y_train_scaled)
      train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size= config.batch_size, shuffle = True)

      # start training
      val_loss = []
      batch_loss = []

      # Iterate the optimizer.
      # tqdm creates a progress bar

      #for e in range(epochs):
      for e in tqdm(range(config.epochs)):
          for X_batch, y_batch in train_data_loader:
              # put all data on device
              X_batch = X_batch.to(device)
              y_batch = y_batch.to(device)
              y_pred = net(X_batch).to(device)

              # zero the optimizer for each batch
              optimizer.zero_grad()

              #loss = loss_function(y_batch, y_pred, config.reg_weight, net.parameters())
              # compute the loss and step the optimizer
              loss = loss_function(y_batch, y_pred)
              # print(loss)
              # sys.exit(3)
              loss.backward()
              optimizer.step()

              # log the batch loss
              wandb.log({"batch_loss":loss.item()})
              batch_loss.append(loss.item())

          # Evaluate the test loss and append it on the list `test_loss`
          y_pred_val = net(X_val_scaled.to(device))
          v_loss = loss_function(y_val_scaled.to(device), y_pred_val.to(device))
          val_loss.append(v_loss.item())
          wandb.log({"val_loss":v_loss.item()})

      wandb.log({"val_loss_end":v_loss.item()})
      # Make a TrainedModel
      trained_model = TrainedModel(net, standarized=standarize,
                                  feature_scaler=feature_scaler,
                                  target_scaler=target_scaler)

      # Return everything we need to analyze the results
      return

# define hyparameter sweep
sweep_config = {
    'method': 'bayes'
    }
metric = {
    'name': 'val_loss_end',
    'goal': 'minimize'
    }
sweep_config['metric'] = metric

parameters_dict = {
    'layers':{
        'distribution':'int_uniform',
        'max': 100,
        'min': 2
    },

    'neurons':{
        'distribution':'int_uniform',
        'max': 100,
        'min': 2
    },

    'activation':{
        'distribution': 'categorical',
        'values': ['ReLU']
    },

    'optimizer': {
        'values': ['adam']
    },

    'learning_rate':{
        'distribution':'log_uniform',
        'max': -5,
        'min': -11
    },

    'batch_size':{
        'distribution':'q_log_uniform',
        'max': 13,
        'min': 4
    },

    'epochs':{
        'distribution':'int_uniform',
        'max': 35,
        'min': 25
    }
    }

sweep_config['parameters'] = parameters_dict

sweep_id = wandb.sweep(sweep_config)
count = 10 # number of runs to execute
wandb.agent(sweep_id, function=train_net, count=count)

wandb.finish()
## Don't run this until we're happy with the model performance 
#path_saved_model = 'savemodel.pth'
#torch.save(model.state_dict(), path_saved_model)